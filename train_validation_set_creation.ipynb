{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93100934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "adbb0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uploads names of the image files in a list for shuffling\n",
    "## Make sure that the images are in the current working directory\n",
    "files=[]\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if '.jpg' in file:\n",
    "        files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "817eb627",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function shuffles the images inplace\n",
    "random.shuffle(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3c49dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splits the images into 80% training, 10% validation and 10% testing\n",
    "split_1 = int(0.8 * len(files))\n",
    "split_2 = int(0.9 * len(files))\n",
    "train_filenames = files[:split_1]\n",
    "val_filenames = files[split_1:split_2]\n",
    "test_filenames = files[split_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "745bed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creates train, validation and test folders\n",
    "if not os.path.exists('train'):\n",
    "    os.makedirs('train')\n",
    "if not os.path.exists('validation'):\n",
    "    os.makedirs('validation')\n",
    "if not os.path.exists('test'):\n",
    "    os.makedirs('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resize and upload the images to the respective folders\n",
    "## Training set\n",
    "for file in train_filenames:\n",
    "    try:\n",
    "        image = Image.open(file)\n",
    "        new_image = image.resize((224, 224))\n",
    "        new_image.save('train/'+file)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a93920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resize and upload the images to the respective folders\n",
    "## Validation set\n",
    "for file in val_filenames:\n",
    "    try:\n",
    "        image = Image.open(file)\n",
    "        new_image = image.resize((224, 224))\n",
    "        new_image.save('validation/'+file)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c0b23",
   "metadata": {},
   "source": [
    "Create the metadata file(lst file). The file should be in the format of\n",
    "\n",
    "<b>image_index, label, image_name/image_path</b>\n",
    "\n",
    "Since this is a multilabel classification problem \n",
    "\n",
    "<b>image_index, label_1, label_2,..label_n, image_name/image_path</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "19156a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame()\n",
    "filenames=[]\n",
    "labels=[]\n",
    "for file in os.listdir('train'):\n",
    "    filenames.append(file)\n",
    "    if 'cloudy' in file:\n",
    "        labels.append(0)\n",
    "    elif 'rain' in file:\n",
    "        labels.append(1)\n",
    "    elif 'shine' in file:\n",
    "        labels.append(2)\n",
    "    else:\n",
    "        labels.append(3)\n",
    "        \n",
    "train_df['labels']=labels\n",
    "train_df['s3_path']=filenames\n",
    "\n",
    "train_df=train_df.reset_index().rename(columns={'index':'row_id'})\n",
    "\n",
    "train_df=pd.concat([train_df,pd.get_dummies(train_df['labels'],prefix='label')],axis=1).drop('labels',axis=1)\n",
    "new_columns=['row_id','label_0','label_1','label_2','label_3','s3_path']\n",
    "train_df=train_df.reindex(columns=new_columns)\n",
    "\n",
    "train_df.to_csv('train.lst',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "de3907b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df=pd.DataFrame()\n",
    "filenames=[]\n",
    "labels=[]\n",
    "for file in os.listdir('validation'):\n",
    "    filenames.append(file)\n",
    "    if 'cloudy' in file:\n",
    "        labels.append(0)\n",
    "    elif 'rain' in file:\n",
    "        labels.append(1)\n",
    "    elif 'shine' in file:\n",
    "        labels.append(2)\n",
    "    else:\n",
    "        labels.append(3)\n",
    "        \n",
    "validation_df['labels']=labels\n",
    "validation_df['s3_path']=filenames\n",
    "\n",
    "validation_df=validation_df.reset_index().rename(columns={'index':'row_id'})\n",
    "\n",
    "val_df=pd.concat([validation_df,pd.get_dummies(validation_df['labels'],prefix='label')],axis=1).drop('labels',axis=1)\n",
    "new_columns=['row_id','label_0','label_1','label_2','label_3','s3_path']\n",
    "val_df=val_df.reindex(columns=new_columns)\n",
    "\n",
    "val_df.to_csv('validation.lst',sep='\\t',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2247a0d",
   "metadata": {},
   "source": [
    "Once the training and testing sets along with the metadata files are available upload them to the S3 buckets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
